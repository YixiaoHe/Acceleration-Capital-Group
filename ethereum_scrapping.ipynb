{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import cfscrape\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = cfscrape.create_scraper()  \n",
    "proxies={\n",
    "    'http':'http://35.169.156.54:3128',\n",
    "    'https':'https://35.169.156.54:3128',\n",
    "}\n",
    "t_id=[] #each transaction id \n",
    "t_timestamp=[] #timestamp for each transaction\n",
    "\n",
    "\n",
    "t_amount=[] #each transaction prices\n",
    "\n",
    "t_block=[] #the block each transaction belongs to\n",
    "t_blockset=set() #counts the number of different blocks number to make sure we webscrape the most recent 100 transactions\n",
    "\n",
    "for page in range(1,301): #each block at most contains 300 transactions, and each page contains 100 transactions, so we need to go over at most 300 pages \n",
    "    url= 'http://etherscan.io/txs?ps=100&p='+str(page)\n",
    "    htmltext = scraper.get(url)\n",
    "\n",
    "    if htmltext.status_code == 200:\n",
    "        soup_object = BeautifulSoup(htmltext.content, 'html.parser')\n",
    "   \n",
    "    flag=0\n",
    "    for p in soup_object.tbody.find_all('a'):\n",
    "        p2=p.get('href')\n",
    "        if p2:\n",
    "            if 'block' in p2:\n",
    "                t_blockset.add(p2.replace('/block/',''))\n",
    "                if len(t_blockset)==101:\n",
    "                    flag=1\n",
    "                    break\n",
    "                \n",
    "                t_block.append(p2.replace('/block/','')) #include all blocks' number into t_block\n",
    "\n",
    "        p1=p.get('class')\n",
    "        \n",
    "        if p1:\n",
    "            if 'myFnExpandBox_searchVal' in p1:\n",
    "                id_temp=p.get('href')\n",
    "                t_id.append(id_temp.replace('/tx/','')) #include all transactions' id into t_id\n",
    "\n",
    "    if flag==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_urlid=[] #each transaction id's website\n",
    "t_source=[] #the source address of each transaction\n",
    "t_destination=[] #the destination address of each transaction\n",
    "for x in t_id:\n",
    "    url_id= 'https://etherscan.io/tx/'+str(x)\n",
    "    t_urlid.append(url_id)\n",
    "    htmltextid = scraper.get(url_id)\n",
    "    if htmltextid.status_code == 200:\n",
    "        soup_objectid = BeautifulSoup(htmltextid.content, 'html.parser')\n",
    "    \n",
    "    for t in soup_objectid.find_all('a'):\n",
    "        t1=t.get('id')\n",
    "        if t1:\n",
    "            if t1=='addressCopy':\n",
    "                t1_addresstem=t.get('href')\n",
    "                t_source.append(t1_addresstem.replace('/address/','')) #include all transactions' address into t_source\n",
    "            elif t1=='contractCopy':\n",
    "                t1_destinationtemp=t.get('href')\n",
    "                t_destination.append(t1_destinationtemp.replace('/address/','')) #include all transactions' destination into t_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get transactions' timestamp and transanction amount\n",
    "t_urldetails=[] #store each transactions' id corresponding websites \n",
    "for i in t_id:\n",
    "    url_details= 'https://etherscan.io/tx/'+ str(i)\n",
    "    t_urldetails.append(url_details)\n",
    "    htmltextd = scraper.get(url_details)\n",
    "    if htmltextd.status_code == 200:\n",
    "        soup_objectd = BeautifulSoup(htmltextd.content, 'html.parser')\n",
    "\n",
    "    for t in soup_objectd.find_all('span'):\n",
    "        t1=t.get('id')\n",
    "\n",
    "        if t1:\n",
    "            if 'ContentPlaceHolder1_spanValue' in t1:\n",
    "                amount_initial=t.find(text=lambda text: text and \"$\" in text)\n",
    "                amount_temp=amount_initial.replace(' (','')\n",
    "                amount=amount_temp.replace(')','')\n",
    "                t_amount.append(amount) #include all transactions' amount into t_amount\n",
    "            elif 'clock' in t1:\n",
    "                t_temp=t.nextSibling\n",
    "                t_tempNext=t_temp.nextSibling\n",
    "                time_temp=re.sub(r'.*ago ','',t_tempNext)\n",
    "                time_temp1=time_temp.replace('\\n','')\n",
    "                time=time_temp1.strip('()')\n",
    "                t_timestamp.append(time) #include all transactions' timestamp into t_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get transactions' mining pools\n",
    "t_blockdetails=[]\n",
    "t_pool=[]\n",
    "count=0\n",
    "for i in t_block:\n",
    "    url_blockdetails='https://etherscan.io/block/'+str(i)\n",
    "    t_blockdetails.append(url_blockdetails)\n",
    "    htmltextd = scraper.get(url_blockdetails)\n",
    "    if htmltextd.status_code == 200:\n",
    "        soup_objectd = BeautifulSoup(htmltextd.content, 'html.parser')\n",
    "    \n",
    "    for h in soup_objectd.find_all('b'):\n",
    "        h_text=h.getText()\n",
    "        t_pool.append(h_text) #include all transactions' mining pools into t_pool\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_listtemp=zip(t_id,t_block,t_timestamp,t_source,t_destination,t_amount,t_pool)\n",
    "whole_list1=list(whole_listtemp)\n",
    "whole_list=sorted(whole_list1, key=lambda x: x[1], reverse=True) #sort transactions based on blocks from the latest to the oldest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('ethereum_data1.csv', 'w') as fp:\n",
    "    writer = csv.writer(fp, delimiter = ',')\n",
    "    writer.writerow([\"transaction_ID\",\"block\" ,\"transaction_timestamp\", \"source_address\",\"destination_address\",\"transaction_amount\",\"mining_pool\"])\n",
    "    for row in whole_list:\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
